# XMIDI Dataset

### A large-scale symbolic music dataset with emotion and genre labels.

Please refer to the following paper for more details:

<blockquote>
<p align="center"><strong>XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework</strong></p>
<p align="center">Sida Tian<sup>♪</sup>, Can Zhang<sup>♪</sup>, Wei Yuan<sup>♪</sup>, Wei Tan, Wenjie Zhu</p>
<p align="center">[<a href="https://arxiv.org/pdf/2501.08809" rel="nofollow">PDF</a>] [<a href="https://arxiv.org/abs/2501.08809" rel="nofollow">ArXiv</a>] [<a href="https://xmusic-project.github.io/" rel="nofollow">Project Page</a>]</p>
</blockquote>

## Brief Introduction

We built XMIDI, the largest known symbolic music dataset with precise emotion and genre labels, comprising 108,023 MIDI files. The average duration of the music pieces is around 176 seconds, resulting in a total dataset length of around 5,278 hours.

## Download

The XMIDI dataset can be downloaded via [google drive](https://drive.google.com/file/d/1qDkSH31x7jN8X-2RyzB9wuxGji4QxYyA/view?usp=sharing).

The zip file contains 108,023 .midi files, and the file naming format is as follows:

```
XMIDI_<Emotion>_<Genre>_<ID_len_8>.midi
```

## Citation

Please consider citing the following paper if you feel our XMIDI dataset useful to your research:


```
@article{xmusic2025,
    title={XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework},
    author={Tian, Sida and Zhang, Can and Yuan, Wei and Tan, Wei and Zhu, Wenjie},
    journal={IEEE Transactions on Multimedia},
    year={2025},
    publisher={IEEE}
}
```
